% !TEX root = ./thesis.tex

\chapter{Related work}
\label{cha:rewo}
Stacked conv AE \cite{Masci2011} introduce Convolutional Auto-Encoder for hierarchical feature extraction.
They claim, that initializing CNN with convolutional layers of trained CAE consistently increases perfromance of a CNN.

VAE. Tutorial on VAE \cite{Doersch2016}

Rationalizing Neural Prediction \cite{Lei2016}


\textit{recommended by tutors}
NG Sparse AE  \cite{Ng2011} introduces notion of sparcity properly
Learning Deep Architectures for AI \cite{Bengio2009}
Stacked convolutional auto-encoders for hierarchical feature extraction \cite{Masci2011}

Spatial Transformer Networks \cite{Jaderberg2015}

Loop closure detection for visual SLAM systems using deep neural networks \cite{Gao2015}
Authors build a denoising autoencoder with sparce objective adding continuity objective.
Continuity objective enforces L2 similaraty between extracted features for consequtive frames.
They use dataset: freiburg2_slam

Deep Convolutional Inverse Graphics Network\cite{Kulkarni2015}
Based on idea of VAE XXX build a network that extracts features related to spatial transformations of 3D objects.

Understanding Visual Concepts with Continuation Learning \cite{Whitney2016}
Authors propose a neural network archtecture for extracting a subset of highly interpretable features
from image sequencies.

Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion \cite{VincentPASCALVINCENT2010}



Autoencoders


Notion of sparcity


Localization issues
