% !TEX root = ./thesis.tex

\chapter{Technical details}
\label{ch:tede}

This chapter provides technical details relevant to the method proposed in chapter \ref{ch:mode}.
In subsection \ref{ch:nn} we provide overview of artificial neural networks and related optimization techniques.
Subsection \ref{ch:ae} describes several unsupervised learning techniques for artificial neural networks also known as autoencoders.

\section{Neural Networks}
\label{ch:nn}

% \subsection{Artificial Neural Network}
\subsection{Convolutional Neural Networks}
\label{ch:cnn}

\subsection{Optimization techniques}
\subsection{Regularization}






\section{Autoencoders}\label{ch:ae}
An autoencoder or autoassociator neural network is an unsupervised learning algorithm that sets target output values equal to input values $y_i=x_i$ \cite{Ng2011,RanzatoMarcAurelio2007}.
Autoencoders are usually trained according to \textit{encoder-decoder} paradigm.
Which first allows to project or encode input $x_i$ into some useful feature representation $h_i$.
Then reconstruct or decode original input $\hat{y_i}$ from representation $h_i$.
This process is schematically on image \ref{fig:ae}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{ae.png}
  \caption{Autoencoder.}
  \label{fig:ae}
\end{figure}



We are going to refer to encoder as a deterministic differentiable function $f(x, \theta_f)$ that for a given set of parameters $\theta_f$ maps input $x\in \Bbb{R}^d$ into representation $h \in \Bbb{R}^{d'}$.
Likewise, decoder is deterministic differentiable function $g(h, \theta_g)$ depending on parameters $\theta_g$ maps $h\in\Bbb{R}^{d'}$ into $y\in \Bbb{R}^d$. We will further avoid specifying parameters $\theta_f$ and $\theta_g$ for simplicity.
Given a loss function $L$, for a dataset $X=\{x_0, ..., x_i\}$ we can define learning objective as follows \cite{Good2016}:

\begin{equation}\label{eq:ae}
\min_{\theta_f, \theta_g}\sum\limits_{x_i \in X}{L(x_i, g(f(x))}
\end{equation}

Learning identity mapping itself is not particularly useful.
Instead we are often interested in reusing only parts of the autoencoder.
For example, parameters of the encoder $\theta_f$ can serce as a good initialization of a dicriminative model \cite{Masci2011, Vincent2010, Zhao2015}.
Such initialization results in minor yet coherent improvement of generalization properties of the model.
Variational Autoencoders, on the other hand, reuse decoder as a generator of new training examples \cite{Kingma2013}.
Extracted features $h$ can represent interpretable properties of the input and can be used to adjust this
properties during generation process \cite{Kulkarni2015, Whitney2016}.

Learning useful feature representation is not a trivial task.
One of the common issues with training autoencoders is possibility to learn identity mapping of input data.
Learning identity mapping would result in less informative feature representation of the inputs.
For example, we can represent both encoder and decoder as a linear function and set the size of the feature space so that it exceeds the size of input space $d' > d$.
Then components of autoencoder can both learn simple identity mapping from input space into feature space and wise-versa.
Therefore, it is common to either limit capacity of the feature space to be less than input space or impose additional regularization on extracted features.

Additionally, both encoder and decoder can be represented as arbitrary neural network.
If capacity of any of two networks is hight enough, it would again lead to learning of identity mapping.
On the other hand, if capacity is chosen too low it can hurt the learning process.
For example, low capacity of generator network in Variational Autoecoders can result in mode collapse.

One of the ways to address the problem of overly high capacity of components can be using stacked autoencoders.
In this approach, instead of directly mapping $h=f(x)$ and $y=g(h)$ into desired lower-dimensional space multiple intermediate mappings can be used $h_i=f_i(h_{i-1})$ and $y_i=g_i(y_{i-1})$.
Stacked autoencoder can be trained in layer-wise manner.
In this case first mappings $y=g_0(h_0)$ and $h_0=f_0(x)$ are trained.
Then follows training of mappings $y_1=g_1(h_1)$ and $h_1=f_0(h_0)$ and so forth.
Architecture of stacked autoencoder is presented on figure \ref{fig:sae}
% \ref{fig}

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{sae.png}
  \caption{Stacked autoencoder.}
  \label{fig:sae}
\end{figure}

In subsection \ref{ch:dcae} we discuss techniques specifically useful in computer vision.
That includes application of convolutional layers and mixing training data with random noise
Subsection \ref{ch:sae} describes possible modifications of the learning objective.


\subsection{Autoencoders and computer vision}\label{ch:dcae}

Selecting a suitable model for computer vision task can be a tedious process.
Input data is often highly dimensional and contains hundreds of thousands of features for each input \ref{ILSVRC15}.
Model has to be flexible enough to address the issue of highly nonlinear relation between input features.
At the same time it has to be highly invariant to position of visual features, spatial transformations and noisy data.
All of this requirements lead to increased of model complexity.

In this subsection we discuss the means of increasing generalization properties of complex computer vision models and modification, more specific to autoencoders.

\subsubsection{Denoising autoencoders}

Learning high capacity models is prone to overfitting.
One of the common ways to decrease overfitting and make model learn useful features without actually changing the model is learning on a larger training corpus.

Denoising autoencoder approach \ref{Vincent2010} allows to increase variation in training data by adding random noise to images. We construct input examples by adding random noise to the dataset according to some parameter $alpha$: $\hat{x}=noise(x, \alpha)$.
Denoising autoencoder is yet supposed to reconstruct  the original image $L(x, g(f(\hat{x})))$.
This approach expects encoded representation to be stable and robust under condition of input corruption.
It also expects that denoising tasks would result in learning useful structure of input distribution.
Denoising autoencoders are reported to allow better generalization, when encoded features are provided to another model to perform classification task.

\subsubsection{Convolutional autoencoders}

One of the issues commonly encountered in computer vision tasks is spatial correlation of the data.
Neighboring pixels of a single image are rarely fully independent.
Fully-connected networks are ill-fitted for addressing such local correlation of data.
Furthermore, high dimensionality of computer vision data results in over-parametrization of such models and leads to overfitting.
Recent trends are clearly skewed towards convolutional models \cite{He2015, Szegedy2016}.
Convolutional models are consistently showing best results in computer vision competitions \cite{ILSVRC15, Zhou2016}.

Convolutional and maxpooling layers provide means of dimensionality reduction.
As described in section \ref{ch:cnn}, applying strided convolutions or pooling layers leads to decrease in resolution of the feature map.
Therefore convolutional and maxpooling layers are natural candidates for feature extraction in encoder network.

Convolutional networks depend on relatively small number of parameters.
This quality is desirable for learning useful feature mapping since it allows to avoid overfitting and, as a result, learning of identity mapping by the network.
As described, number of parameters of convolutional layer depends only on kernel size and depth of the feature map.
Number of parameters required by pooling layers is relatively small if not zero and does not depend on the size of the input.

\subsubsection{Deconvolutional layers and input reconstruction}

While convolutional layers are natural for feature extraction in encoder, reconstruction of highly dimensional input requires a different technique.
One approach is to use an inverse-like operation to convolution.
Deconvolution layer, also known as transposed convolution, is one possible approach.

Deconvolutional layers \cite{Zeiler2010} take as an input an image $y$ composed of $K_0$ color channels $y_1, ... , y_{K_0}$.
As a result of deconvolution process we would like obtain $K_1$ feature maps of the same image.
We can represent each cahnnel $c$ of $K_0$ channels as $K_1$ channels convolved with feature maps $f_{k,c}$:

\begin{equation}\label{eq:de}
  \sum^{K_1}_{k=1}=z^i_k \cdot f_{k,c} = y^i_{k,c}
\end{equation}

where $\cdot$ is a dot product.

Using this equation we would like to find latent feature maps $f_{k,c}$.
Since equation \ref{eq:de} is an under-determined and allows multiple solution.
Therefore a regularization term $z^i_k$ is added to encourage sparsity of the solutions.
With sparsity term we can express the cost function $C$ for input $y_i$:

\begin{equation}\label{eq:dec}
    C(y_i) = \frac{\lambda}{2} \sum^{K_o}_{c=1} ||\sum^{K_1}_{k=1}{z^i_k \cdot f_{k,c} - y^i_{k,c}}||^2_2 + \sum^{K_1}_{k=1}{|z^i_k|^p}
\end{equation}

where we assume mean square error as a reconstruction cost and $p$ as regularization norm.
Constant $\lambda$ balances contributions to the total cost between reconstruction and regularization terms.

Problem \ref{eq:dec} for simplicity can be decomposed into 2 sequential quadratic problems and solved using stochastic gradient descent method.

\subsubsection{Unpooling layers}

Like with convolutional layers, applying pooling layers results in change of shape of feature map.
In this subsection we describe a method that allows to effectively revert max-pooling layer preserving relative spatial position of extracted features as on original image.

As described in section \ref{ch:cnn} maxpooling layer with stride $k$.
Applying this layers allows to reduce size of feature map $k^2$ times while preserving arguable most important information about extracted features.
Max-pooling is crucial for convolutional neural networks for achieving position tolerance.

Inverse operation of max-pooling is not well defined, since maximum value must be placed somewhere inside original volume of size $k^2$. Naively, we can choose an index of a cell index inside projection volume and put maximum values uniformly into cells with that index.
This procedure is depicted on figure \ref{fig:pool}.
However, this approach destroes relative positioning of unpooled features.
In other words, pooling allows higher lever features to recognize a face by presence of a nose and a mouse within neighboring smaller regions, naive unpooling might place both this features into the same region on the image.

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{pool.png}
  \caption{Pooling layer and naive inverse operation.}
  \label{fig:pool}
\end{figure}

To address this issue unpooling layer is introduced. Instead of using information about only maximum value of the feature in the region, unpooling layer also uses information about positioning of the feature. This positional information is called a mask. To every pooled value $x_{i,j}$ corresponds a single mask of size equal to pooling region size. To use mask information  unpooling layer take as an input volume with the same shape as produced by pooling layer. As follows, pooling layer describe in section \ref{ch:cnn} must also extract information about position of maximum value. This modification does not otherwise effect the layer nether during forward nor during backward pass. Application of unpooling layer is illustrated on figure \ref{fig:unpool}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{unpool.png}
  \caption{Illustration of pooling and unpooling layers.}
  \label{fig:unpool}
\end{figure}

Unpooling layers contain no parameters and is not effected by backpropagation.

It worth noting, that mask information extracted by modified pooling layer depends on order of feature channels. As stated by the authors of the original algorithm, this might affect features, extracted by layers preceeding unpooling \cite{Zhao2015}.

\subsubsection{What-Where autoencoders}



\subsection{Extracting usefull features with Autoencoders}\label{ch:sae}

Standalone objective \ref{eq:ae} of autoencoder is not particularly useful.
In this section we are going to describe specific use-cases of applying autoencoders along with required modification of objective function.

\subsubsection{Sparse Autoencoders}\label{ch:vae}

Lets consider activations in the hidden layer $h$ for some input $x$:
\begin{equation}
  h(x) = \sigma(W_{h}x_{-1} + b_{h})
\end{equation}
where $W_h$ and $b_h$ are parameters of hidden layer $h$, $x_{-1}$ is output of the previous hidden layer and $\sigma$ is some activation function.
For example for sigmoid activation function every component of vector $h(x)$ would belong to interval $[0, 1]$.
Sparse autoencoders introduce additional term $l_{sparse}$ to loss function to add a constrain on the activations of the hidden layer $h$ \cite{Ng2011}:
\begin{equation}
  l_{total}(x) = l_{reconstruction}(x, g(f(x))) + \alpha*l_{sparse}(x, h(x))
\end{equation}
where $\alpha$ is a constant parameter.

Sparsity constrain tries to achieve low average activation $\hat{\rho}$ on the hidden layer:

\begin{equation}\label{eq:avgh}
  \rho = \sum_{i=1}^N h_i(x)/N
\end{equation}

For example, lets take a look at the case for sigmoid activation function in the hidden layer.
Sigmoid produces outputs in the interval $[0, 1]$. If we can guarantee average activation on the hidden layer $\hat{\rho} <= 0.1$ we can rely upon the fact that no more than $10\%$ of hidden neurons would have activatoin close to $1.0$.

Sparse constrain allows to extract useful features even for relatively high number of hidden units. Specifically, decoder must be able to reconstruct original representation relying only on few high activation on the hidden layer.

There are several ways to enforce sparsity constrain.
It is possible to perform gradient descent directly for fixed value of $\hat{\rho}$ but we would have to perform a second pass over the dataset to calculate actual value of $\rho$. Instead we can simply nullify all but $k$ highest activation during the forward path through the network \cite{Kulkarni2015}.
Or apply negative log-likelyhood to the hidden layer of the network \cite{Zhao2015}.
At last, to achieve true zero values on the hidden layer we can use absolute value penalty $l_{sparce}=\sum_{i=1}^N |h_i|$ in conjunction with rectifier linear unit activation function \cite{Glorot2011}.

Sparse coding of the data has proven to be useful to solve classification problem.
In particular, for classification task requires us to assign one of $C$ exclusive classes we can train an autoencoder with $C$ hidden units. Once training converges we expect that features, learned by the hidden layer would somewhat correspond to classes of tasks. We can then use encoder part of the network as initialization of a classifier of the same architecture \cite{Masci2011}.
Such initialization leads to consistently better generalization results after supervised training of the classifier.

\subsubsection{Variational Autoencoders}\label{ch:vae}



% \subsection{Generative adcersarial nets}\label{ch:gan}
% \section{Autoencoders and feature interpretability}
